{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzzy Clustering with Neural Networks\n",
    "\n",
    "This notebook implements customer categorization using fuzzy clustering with neural network approaches.\n",
    "\n",
    "## Objectives\n",
    "1. Load processed customer data\n",
    "2. Build a neural network-based fuzzy clustering model\n",
    "3. Train and evaluate the model\n",
    "4. Compare with traditional ML approach\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "DATA_DIR = Path('../data/processed')\n",
    "\n",
    "# TODO: Load your processed customer data\n",
    "# df = pd.read_csv(DATA_DIR / 'customers_cleaned.csv')\n",
    "# print(f\"Loaded {len(df)} customer records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select features and normalize\n",
    "# features = ['feature1', 'feature2', 'feature3']  # Select relevant features\n",
    "# X = df[features].values\n",
    "\n",
    "# Standardize features\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data for validation\n",
    "# X_train, X_val = train_test_split(X_scaled, test_size=0.2, random_state=42)\n",
    "# print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Fuzzy Neural Network Model\n",
    "\n",
    "Create an autoencoder-based architecture for fuzzy clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build neural network model\n",
    "# n_features = X_scaled.shape[1]\n",
    "# n_clusters = 5\n",
    "# encoding_dim = 10\n",
    "\n",
    "# # Encoder\n",
    "# encoder_input = layers.Input(shape=(n_features,))\n",
    "# encoded = layers.Dense(64, activation='relu')(encoder_input)\n",
    "# encoded = layers.Dense(32, activation='relu')(encoded)\n",
    "# encoded = layers.Dense(encoding_dim, activation='relu')(encoded)\n",
    "\n",
    "# # Clustering layer (soft assignment)\n",
    "# cluster_layer = layers.Dense(n_clusters, activation='softmax', name='cluster_output')(encoded)\n",
    "\n",
    "# # Decoder\n",
    "# decoded = layers.Dense(32, activation='relu')(encoded)\n",
    "# decoded = layers.Dense(64, activation='relu')(decoded)\n",
    "# decoder_output = layers.Dense(n_features, activation='linear', name='reconstruction_output')(decoded)\n",
    "\n",
    "# # Create model\n",
    "# model = models.Model(inputs=encoder_input, outputs=[cluster_layer, decoder_output])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compile and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compile model\n",
    "# model.compile(\n",
    "#     optimizer='adam',\n",
    "#     loss={\n",
    "#         'cluster_output': 'categorical_crossentropy',\n",
    "#         'reconstruction_output': 'mse'\n",
    "#     },\n",
    "#     loss_weights={\n",
    "#         'cluster_output': 0.5,\n",
    "#         'reconstruction_output': 0.5\n",
    "#     },\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# # Note: For unsupervised clustering, you may need to use a custom training loop\n",
    "# # or pretrain with autoencoder reconstruction only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train model\n",
    "# # Early stopping callback\n",
    "# early_stopping = keras.callbacks.EarlyStopping(\n",
    "#     monitor='val_loss',\n",
    "#     patience=10,\n",
    "#     restore_best_weights=True\n",
    "# )\n",
    "\n",
    "# # Train model\n",
    "# history = model.fit(\n",
    "#     X_train,\n",
    "#     [X_train, X_train],  # For unsupervised learning\n",
    "#     epochs=100,\n",
    "#     batch_size=32,\n",
    "#     validation_data=(X_val, [X_val, X_val]),\n",
    "#     callbacks=[early_stopping],\n",
    "#     verbose=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot training history\n",
    "# plt.figure(figsize=(12, 4))\n",
    "\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(history.history['loss'], label='Training Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.title('Model Loss')\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.plot(history.history['reconstruction_output_loss'], label='Reconstruction Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.title('Reconstruction Loss')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cluster Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get cluster assignments\n",
    "# cluster_probs, _ = model.predict(X_scaled)\n",
    "# cluster_labels = np.argmax(cluster_probs, axis=1)\n",
    "\n",
    "# df['cluster_nn'] = cluster_labels\n",
    "\n",
    "# # Add membership probabilities\n",
    "# for i in range(n_clusters):\n",
    "#     df[f'nn_membership_cluster_{i}'] = cluster_probs[:, i]\n",
    "\n",
    "# print(df['cluster_nn'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cluster Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize clusters\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, \n",
    "#                       cmap='viridis', alpha=0.6, s=50)\n",
    "# plt.colorbar(scatter)\n",
    "# plt.xlabel('First Principal Component')\n",
    "# plt.ylabel('Second Principal Component')\n",
    "# plt.title('Customer Segments (Neural Network)')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare with Traditional ML Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load traditional ML results and compare\n",
    "# df_fcm = pd.read_csv(DATA_DIR / 'customers_with_clusters_fcm.csv')\n",
    "\n",
    "# # Compare cluster distributions\n",
    "# print(\"Traditional ML cluster distribution:\")\n",
    "# print(df_fcm['cluster'].value_counts())\n",
    "# print(\"\\nNeural Network cluster distribution:\")\n",
    "# print(df['cluster_nn'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save results\n",
    "# OUTPUT_DIR = Path('../data/processed')\n",
    "# df.to_csv(OUTPUT_DIR / 'customers_with_clusters_nn.csv', index=False)\n",
    "\n",
    "# # Save model\n",
    "# model.save('../models/fuzzy_clustering_nn.h5')\n",
    "# print(\"Results and model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "1. Evaluate which approach (traditional ML vs NN) works better for your data\n",
    "2. Fine-tune hyperparameters\n",
    "3. Validate clusters with business stakeholders\n",
    "4. Deploy the model for production use"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
